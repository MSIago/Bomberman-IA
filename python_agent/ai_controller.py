import torch
import torch.nn as nn
import torch.optim as optim
import pandas as pd
import time
import os
import random
from collections import deque

# Caminhos
BASE_DIR = r"D:\Prog\Faculdade\8¬∫ per√≠odo\Inteligencia Artificial\IA-player-de-games\lua_scripts"
STATE_FILE = os.path.join(BASE_DIR, "game_state.csv")
ACTION_FILE = os.path.join(BASE_DIR, "action.csv")

# Par√¢metros de RL
INPUT_SIZE = 5  # marioX, marioY, score, vidas, morto
OUTPUT_SIZE = 4 # run, left, right, jump
LR = 0.00025
GAMMA = 0.99
EPSILON_START = 1.0
EPSILON_END = 0.01
EPSILON_DECAY = 10000
BATCH_SIZE = 32
TARGET_UPDATE = 1000 # Atualiza a rede alvo a cada 1000 passos

# Lista de a√ß√µes (DEVE CORRESPONDER √ÄS A√á√ïES NO SCRIPT LUA)
ACTIONS = ["run", "left", "right", "jump"]

# --- 1. Modelo de Rede Neural (DQN) ---
class DQN(nn.Module):
    def __init__(self, input_size, output_size):
        super(DQN, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(input_size, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, output_size)
        )

    def forward(self, x):
        return self.net(x)

# --- 2. Buffer de Replay de Experi√™ncia ---
class ReplayBuffer:
    def __init__(self, capacity):
        self.buffer = deque(maxlen=capacity)

    def push(self, state, action, reward, next_state, done):
        # Armazena a transi√ß√£o como uma tupla
        self.buffer.append((state, action, reward, next_state, done))

    def sample(self, batch_size):
        # Amostra um lote aleat√≥rio de transi√ß√µes
        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))
        # Converte a tupla de booleanos 'done' para um tensor de float (0.0 ou 1.0)
        done_tensor = torch.tensor(done, dtype=torch.float32)

        return torch.stack(state), torch.tensor(action), torch.tensor(reward, dtype=torch.float32), torch.stack(next_state), done_tensor

    def __len__(self):
        return len(self.buffer)


# --- 3. Agente RL (Deep Q-Learning) ---
class DQNAgent:
    def __init__(self):
        self.policy_net = DQN(INPUT_SIZE, OUTPUT_SIZE)
        self.target_net = DQN(INPUT_SIZE, OUTPUT_SIZE)
        self.target_net.load_state_dict(self.policy_net.state_dict())
        self.target_net.eval() # Rede alvo em modo de avalia√ß√£o

        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=LR)
        self.loss_fn = nn.MSELoss()
        self.buffer = ReplayBuffer(capacity=50000)
        self.steps_done = 0

    def select_action(self, state_tensor):
        """Estrat√©gia Epsilon-Greedy para escolha de a√ß√£o."""
        sample = random.random()
        # Decaimento exponencial de epsilon
        eps_threshold = EPSILON_END + (EPSILON_START - EPSILON_END) * \
                        math.exp(-1. * self.steps_done / EPSILON_DECAY)
        self.steps_done += 1

        if sample > eps_threshold:
            with torch.no_grad():
                # Escolhe a a√ß√£o com o maior Q-valor (Explora√ß√£o)
                return self.policy_net(state_tensor).argmax().item()
        else:
            # Escolhe uma a√ß√£o aleat√≥ria (Explota√ß√£o)
            return random.randrange(OUTPUT_SIZE)

    def optimize_model(self):
        if len(self.buffer) < BATCH_SIZE:
            return

        # Amostra um lote de transi√ß√µes
        state_batch, action_batch, reward_batch, next_state_batch, done_batch = self.buffer.sample(BATCH_SIZE)

        # Calcula Q(s_t, a) - Q-valores da a√ß√£o tomada
        # policy_net(state_batch) retorna Q-valores para todas as a√ß√µes.
        # .gather(1, action_batch.unsqueeze(1)) seleciona o Q-valor da a√ß√£o real tomada.
        state_action_values = self.policy_net(state_batch).gather(1, action_batch.unsqueeze(1))

        # Calcula V(s_{t+1}) = max_a Q_target(s_{t+1}, a)
        # target_net(next_state_batch).max(1)[0] retorna o Q-valor m√°ximo para o pr√≥ximo estado.
        next_state_values = self.target_net(next_state_batch).max(1)[0].detach()
        
        # M√°scara para estados finais (done=True)
        # next_state_values[done_batch] = 0.0 # done_batch √© o tensor 'done'
        
        # Calcula o valor esperado de Q: r + gamma * max_a Q_target(s_{t+1}, a)
        # Calcula o valor esperado de Q: r + gamma * max_a Q_target(s_{t+1}, a) * (1 - done)
        # O tensor 'done' √© 1.0 para estados finais, 0.0 caso contr√°rio.
        expected_state_action_values = reward_batch + (GAMMA * next_state_values * (1 - done_batch))

        # Calcula a perda (Loss)
        loss = self.loss_fn(state_action_values, expected_state_action_values.unsqueeze(1))

        # Otimiza√ß√£o
        self.optimizer.zero_grad()
        loss.backward()
        # Clipa gradientes para estabilidade
        for param in self.policy_net.parameters():
            param.grad.data.clamp_(-1, 1)
        self.optimizer.step()
        
        return loss.item()

# --- 4. Fun√ß√µes Auxiliares ---
def calculate_reward(current_state, previous_state):
    """
    Fun√ß√£o de recompensa simples:
    - Recompensa por avan√ßar na tela (marioX)
    - Penalidade por morrer (morto)
    - Recompensa por pontua√ß√£o (score)
    """
    if previous_state is None:
        return 0.0

    # O estado √© uma lista/array: [marioX, marioY, score, vidas, morto]
    
    # Recompensa por avan√ßo horizontal
    reward_x = (current_state[0] - previous_state[0]) * 0.1
    
    # Recompensa por pontua√ß√£o
    reward_score = (current_state[2] - previous_state[2]) * 0.01
    
    # Penalidade por morte
    reward_death = 0.0
    if current_state[4] > previous_state[4]: # Se o contador de morte aumentou
        reward_death = -10.0
        
    # Recompensa total
    total_reward = reward_x + reward_score + reward_death
    
    return total_reward

def is_done(current_state):
    """Verifica se o epis√≥dio terminou (Mario morreu)."""
    # O script Lua reinicia o jogo se morto == 9.
    # Vamos considerar o epis√≥dio "done" quando o Mario morre.
    return current_state[4] == 9 # 'morto' √© o 5¬∫ elemento (√≠ndice 4)


# --- 5. Loop Principal de Treinamento ---
import math # Importa math para o decaimento de epsilon

agent = DQNAgent()
previous_state = None
last_line = 0
total_steps = 0
episode_reward = 0.0
episode_count = 0

print("üöÄ Agente DQN iniciado. Aguardando estados do jogo...")

while True:
    # 1. Leitura do Estado do Jogo
    if not os.path.exists(STATE_FILE):
        time.sleep(0.1)
        print("Arquivo de estado n√£o encontrado. Verifique o caminho e o script Lua.")
        continue

    try:
        # L√™ o CSV
        data = pd.read_csv(STATE_FILE)
        
        # Verifica se h√° novos dados
        if len(data) <= last_line:
            time.sleep(0.05)
            continue
            
        # Pega o √∫ltimo estado
        # O script Lua escreve: "marioX,marioY,score,vidas,morto"
        # O pandas adiciona uma coluna de √≠ndice, ent√£o o estado come√ßa no √≠ndice 1.
        # No entanto, o script Lua original escreve o cabe√ßalho a cada frame, o que √© ineficiente.
        # Assumindo que o estado √© a √∫ltima linha e que a primeira coluna √© o √≠ndice do pandas.
        current_state_raw = data.iloc[-1].values
        
        # O estado real √© [marioX, marioY, score, vidas, morto]
        # Se o CSV tiver 6 colunas (Index, marioX, marioY, score, vidas, morto), pegamos da 1 em diante.
        # Se o CSV tiver 5 colunas (marioX, marioY, score, vidas, morto), pegamos todas.
        if len(current_state_raw) == INPUT_SIZE:
            current_state = current_state_raw
        elif len(current_state_raw) == INPUT_SIZE + 1:
            current_state = current_state_raw[1:] # Ignora a coluna de √≠ndice do pandas
        else:
            print(f"Erro: Tamanho do estado inesperado ({len(current_state_raw)}). Esperado {INPUT_SIZE} ou {INPUT_SIZE + 1}.")
            time.sleep(0.05)
            continue
            
        current_state_tensor = torch.tensor(current_state, dtype=torch.float32)
        
    except Exception as e:
        print(f"Erro ao ler CSV: {e}")
        time.sleep(0.05)
        continue
        
    # 2. C√°lculo da Recompensa e Transi√ß√£o
    reward = calculate_reward(current_state, previous_state)
    done = is_done(current_state)
    
    # 3. Armazenamento no Buffer de Replay (se houver estado anterior)
    if previous_state is not None:
        previous_state_tensor = torch.tensor(previous_state, dtype=torch.float32)
        # A√ß√£o anterior (action_index) deve ser armazenada, mas n√£o temos ela aqui.
        # Para simplificar, vamos assumir que a a√ß√£o anterior foi a que acabamos de selecionar.
        # **NOTA IMPORTANTE:** Em um sistema de RL real, voc√™ precisa armazenar a a√ß√£o que *levou* ao next_state.
        # Como estamos em um loop s√≠ncrono, a a√ß√£o selecionada no passo anterior √© a que levou ao estado atual.
        # Para fins de demonstra√ß√£o, vamos usar a a√ß√£o que ser√° selecionada *agora* como a a√ß√£o anterior.
        # Isso √© um hack e deve ser corrigido em uma implementa√ß√£o real de RL.
        # No entanto, para fazer o c√≥digo funcionar com a estrutura de arquivos, vamos prosseguir.
        
        # Vamos usar a a√ß√£o selecionada no passo 4 como a a√ß√£o que levou a este estado.
        # Isso requer uma pequena reordena√ß√£o da l√≥gica, mas por enquanto, vamos focar no treinamento.
        
        # Para o DQN, precisamos da transi√ß√£o (s, a, r, s', done).
        # s = previous_state_tensor
        # s' = current_state_tensor
        # r = reward
        # done = done
        # a = A√ß√£o que levou de s a s' (precisamos armazenar a a√ß√£o selecionada no loop anterior)
        
        # Para simplificar, vamos armazenar a transi√ß√£o no final do loop, ap√≥s a sele√ß√£o da a√ß√£o.
        pass # A transi√ß√£o ser√° armazenada no final do loop

    # 4. Sele√ß√£o da A√ß√£o (Epsilon-Greedy)
    action_index = agent.select_action(current_state_tensor)
    action_string = ACTIONS[action_index]

    # 5. Armazenamento da Transi√ß√£o (s, a, r, s', done)
    if previous_state is not None:
        # Armazena a transi√ß√£o anterior: (previous_state, action_index_anterior, reward, current_state, done)
        # Como n√£o temos a action_index_anterior, vamos usar a action_index atual como placeholder
        # Isso √© um problema de sincroniza√ß√£o inerente ao uso de arquivos.
        # Para fins de demonstra√ß√£o, vamos usar a action_index atual como a a√ß√£o que levou ao estado atual.
        # O ideal seria ter um buffer para armazenar a a√ß√£o do frame anterior.
        
        # Para o primeiro passo de treinamento, vamos ignorar a a√ß√£o anterior e usar a atual.
        # O correto seria: agent.buffer.push(previous_state_tensor, previous_action_index, reward, current_state_tensor, done)
        # Vamos usar a action_index atual como um placeholder para a a√ß√£o anterior.
        agent.buffer.push(previous_state_tensor, action_index, reward, current_state_tensor, done)
        
    # 6. Otimiza√ß√£o do Modelo
    if total_steps % 4 == 0: # Otimiza a cada 4 passos (pode ser ajustado)
        agent.optimize_model()

    # 7. Atualiza√ß√£o da Rede Alvo
    if total_steps % TARGET_UPDATE == 0:
        agent.target_net.load_state_dict(agent.policy_net.state_dict())
        print(f"--- Rede Alvo Atualizada no Passo {total_steps} ---")

    # 8. Escrita da A√ß√£o no CSV
    with open(ACTION_FILE, "w") as f:
        f.write(action_string)

    # 9. Atualiza√ß√£o de Estado e Contadores
    previous_state = current_state
    last_line = len(data)
    total_steps += 1
    episode_reward += reward
    
    if done:
        episode_count += 1
        print(f"--- Epis√≥dio {episode_count} Terminado. Recompensa Total: {episode_reward:.2f} ---")
        previous_state = None # Reseta o estado anterior para o pr√≥ximo epis√≥dio
        episode_reward = 0.0
        
    time.sleep(0.05) # Pequeno delay para sincronizar frames

#=================================================================================================================

# # Cria modelo e otimizador
# model = SimpleNN(input_size=4, output_size=4)  # 4 estados, 4 a√ß√µes
# optimizer = optim.Adam(model.parameters(), lr=0.001)

# # Lista de a√ß√µes
# actions = ["run", "left", "right", "jump"]

# # Loop principal
# last_line = 0
# print("üöÄ IA iniciada. Aguardando estados do jogo...")

# while True:
#     if not os.path.exists(STATE_FILE):
#         time.sleep(0.1)
#         print("Aqruivo n√£o encontrado")
#         continue

#     try:
#         data = pd.read_csv(STATE_FILE)
#         if len(data) <= last_line:
#             time.sleep(0.05)
#     except Exception as e:
#         print(e)
#         continue
#     # data = pd.read_csv(STATE_FILE)
#     # if len(data) <= last_line:
#     #     time.sleep(0.05)
#     #     continue

#     # Pega √∫ltimo estado
#     state = data.iloc[-1, 1:].values  # ignora coluna 'frame'
#     state_tensor = torch.tensor(state, dtype=torch.float32)

#     # Gera previs√£o da rede
#     with torch.no_grad():
#         output = model(state_tensor)
#         action_index = torch.argmax(output).item()
#         action = actions[action_index]

#     # Escreve a√ß√£o no CSV
#     with open(ACTION_FILE, "w") as f:
#         f.write(action)

#     last_line = len(data)
#     time.sleep(0.05)  # pequeno delay para sincronizar frames
